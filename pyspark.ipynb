{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9684d9-f23d-4a10-a4e7-3790d91720d5",
   "metadata": {},
   "source": [
    "## Creation de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ddeb51-ebd5-4c92-9ab4-9f8693e5c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=(SparkSession\n",
    "       .builder\n",
    "       .master(\"local[*]\")\n",
    "       .appName(\"Spark streaming Process\")\n",
    "       .config(\"spark.streaming.stopGracefullyOnShutdown\",True)\n",
    "       .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\")\n",
    "       .config(\"spark.sql.shuffle.partitions\",8)\n",
    "       .getOrCreate()\n",
    "      ) \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc336154-3c0e-41ea-a6fb-56d32dbab42e",
   "metadata": {},
   "source": [
    "# Creation du df qui prend les données depuis Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89760cf-f971-47f9-bf4a-aae666ebf858",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df=(\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\",\"ed-kafka:29092\")\n",
    "    .option(\"subscribe\",\"device-data-3\")\n",
    "    .option(\"startingOffsets\",\"earliest\")\n",
    "    .load()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952212b-021c-46f0-b635-c963338c05fa",
   "metadata": {},
   "source": [
    "# Montre le schema des données prises dans le kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ca0971-56fb-4fb2-8237-f28b18147e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.printSchema()\n",
    "#kafka_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace4f7e9-1a52-4a45-be07-fd0b008dcae1",
   "metadata": {},
   "source": [
    "# Traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ff50d-a0fc-4167-9426-c3e5f7de94bb",
   "metadata": {},
   "source": [
    "## Transformation du type de 'value' de binary vers string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc371ed-9850-4b41-ade8-eed6f6d53441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "kafka_json_df = kafka_df.withColumn(\"value\",expr(\"cast(value as string)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d42106f-536e-43e7-8ebb-51ce3f6a0fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895470b8-8f6d-4679-8b83-7f1a03c60ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kafka_json_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c316e59-556a-4235-a9ae-ac9575c4a751",
   "metadata": {},
   "source": [
    "## Definition du schema des données venant de kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ae5520-0cfc-49f5-aa98-e1b39bef18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "\n",
    "json_schema = (\n",
    "    StructType(\n",
    "    [StructField('customerId', StringType(), True), \n",
    "    StructField('data', StructType(\n",
    "        [StructField('devices', \n",
    "                     ArrayType(StructType([ \n",
    "                        StructField('deviceId', StringType(), True), \n",
    "                        StructField('measure', StringType(), True), \n",
    "                        StructField('status', StringType(), True), \n",
    "                        StructField('temperature', LongType(), True)\n",
    "                    ]), True), True)\n",
    "        ]), True), \n",
    "    StructField('eventId', StringType(), True), \n",
    "    StructField('eventOffset', LongType(), True), \n",
    "    StructField('eventPublisher', StringType(), True), \n",
    "    StructField('eventTime', StringType(), True)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44744e0-862e-497a-8c20-5946a486df82",
   "metadata": {},
   "source": [
    "## Recuperation de la partie 'value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b6fc3d5-5826-4802-ac2e-cd777a4a269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "streaming_df=kafka_json_df.withColumn(\"values_json\",from_json(col(\"value\"),json_schema)).selectExpr(\"values_json.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "727ae32c-148d-40be-874d-2e0435e760f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df.printSchema()\n",
    "#streaming_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c3e801-3701-4c7c-857d-d5b280b12daa",
   "metadata": {},
   "source": [
    "## Explosion de la colonne device avec data_device puis de cette meme colonne en  nouvelle colonne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345d0478-f691-468b-8477-24dfa98de06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "explode_df=streaming_df.withColumn(\"data_devices\",explode(\"data.devices\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95bb278b-752b-4a1c-9a95-a94a82bea62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- data_devices: struct (nullable = true)\n",
      " |    |-- deviceId: string (nullable = true)\n",
      " |    |-- measure: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42016039-b1ac-4a55-b5c4-ce6f5c889cef",
   "metadata": {},
   "source": [
    "## Recuperation des nouvelles colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f8478-6a40-4e55-90ef-58797295a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "flatten_df=(\n",
    "    explode_df\n",
    "    .drop(\"data\")\n",
    "    .withColumn(\"deviceId\",col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\",col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\",col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\",col(\"data_devices.temperature\"))\n",
    "    .drop(\"data_devices\")\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd0b2d53-32b0-4338-be77-5d6467b8cdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatten_df.printSchema()\n",
    "#flatten_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c12eb-8b47-4db6-9af7-8cc89964e48e",
   "metadata": {},
   "source": [
    "## Ecriture en temps reels des données traitées dans un fichier csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87bc2e91-848b-4267-a01d-caa322b0bb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m (\u001b[43mflatten_df\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/output/device_data_1.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m10 seconds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint_dir_kafka_9\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 10\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(flatten_df\n",
    " .writeStream\n",
    " .format(\"csv\")\n",
    " .option(\"path\", \"data/output/device_data_1.csv\")\n",
    " .queryName(\"kafka_table\")\n",
    " .outputMode(\"append\")\n",
    " .trigger(processingTime='10 seconds')\n",
    " .option(\"checkpointLocation\",\"checkpoint_dir_kafka_9\")\n",
    " .start()\n",
    " .awaitTermination())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
