{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "# Traitement des données avec spark",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "markdown",
			"source": "## Importation des bibliothèques",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lit, explode, to_timestamp, hour, date_format\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Initialisation de Spark et Glue",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "sc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Définition des chemins S3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "S3_raw_datas = \"s3://raw-datas-projet/finance/\"\nS3_output = \"s3://raw-datas-projet/output/\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Lecture en streaming des nouvelles données",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_streaming = (\n    spark.readStream\n    .option(\"multiline\", \"true\")\n    .schema(\"results ARRAY<STRUCT<\"  \n            \"id: STRING, name: STRING, symbol: STRING, current_price: DOUBLE, \"\n            \"market_cap: LONG, total_volume: LONG, high_24h: DOUBLE, low_24h: DOUBLE, \"\n            \"price_change_24h: DOUBLE, price_change_percentage_24h: DOUBLE, \"\n            \"ath: DOUBLE, ath_date: STRING, atl: DOUBLE, atl_date: STRING, last_updated: STRING, \"\n            \"roi: STRUCT<currency: STRING, percentage: DOUBLE, times: DOUBLE>>>\")\n    .json(S3_raw_datas)\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Transformation en streaming",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_exploded = df_streaming.withColumn(\"results\", explode(df_streaming[\"results\"]))\n\ndf_flattened = df_exploded.select(\n    \"results.id\",\n    \"results.name\",\n    \"results.symbol\",\n    \"results.current_price\",\n    \"results.market_cap\",\n    \"results.total_volume\",\n    \"results.high_24h\",\n    \"results.low_24h\",\n    \"results.price_change_24h\",\n    \"results.price_change_percentage_24h\",\n    \"results.ath\",\n    \"results.ath_date\",\n    \"results.atl\",\n    \"results.atl_date\",\n    \"results.last_updated\",\n    \"results.roi.currency\",\n    \"results.roi.percentage\",\n    \"results.roi.times\"\n)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Conversion des dates en TimestampType",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "date_columns = [\"ath_date\", \"atl_date\", \"last_updated\"]\nfor col_name in date_columns:\n    df_flattened = df_flattened.withColumn(col_name, to_timestamp(df_flattened[col_name], \"yyyy-MM-dd'T'HH:mm:ss.SSSX\"))\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Extraction des heures et jours pour des analyses dashboard ou calcus d'indicateurs",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_flattened = df_flattened.withColumn(\"hour_updated\", hour(df_flattened[\"last_updated\"]))\ndf_flattened = df_flattened.withColumn(\"day_updated\", date_format(df_flattened[\"last_updated\"], \"EEEE\"))\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Filtrons les cryptos avec un volume de transaction significatif",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_flattened = df_flattened.filter(col(\"total_volume\") > 1000)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Calcul de la volatilité sur 24h ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import expr\n\ndf_flattened = df_flattened.withColumn(\"volatility_24h\", \n                                       expr(\"(high_24h - low_24h) / current_price * 100\"))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Écriture en streaming vers S3 ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "query_csv = (\n    df_flattened\n    .writeStream\n    .format(\"csv\") \n    .option(\"checkpointLocation\", \"s3://raw-datas-projet/checkpoints/\")  \n    .option(\"path\", S3_output)  \n    .option(\"header\", \"true\")  \n    .option(\"delimiter\", \",\")  \n    .outputMode(\"append\")  \n    .start()\n)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "\n## Streaming",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "query_csv.awaitTermination()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "StreamingQueryException: Query [id = d39d586d-b74e-4244-8786-cb64403029b3, runId = cdd4ea7b-74e9-4912-b487-5a2cfab0694b] terminated with exception: Multiple streaming queries are concurrently using s3://raw-datas-projet/checkpoints/sources/0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}